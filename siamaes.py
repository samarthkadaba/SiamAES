# -*- coding: utf-8 -*-
"""siames

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17IjTu0FPgUEYwLcYrTf1-E6rgXlGV7Qx

# Siamese Networks for AES
Author: Sam Kadaba, Abel John
"""

from collections import defaultdict, Counter
import json

from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
import torch
import math
import pickle

from transformers import DistilBertTokenizer, DistilBertModel, DistilBertConfig
import torch.nn as nn
from datasets import load_dataset, DatasetDict
import datasets
from torch.utils.data import DataLoader

from transformers import AdamW, get_linear_schedule_with_warmup
from transformers.modeling_outputs import TokenClassifierOutput
from tqdm.notebook import tqdm
import argparse

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

parser = argparse.ArgumentParser()
parser.add_argument('--prompt', required=True)
parser.add_argument('--n_classes', default=None);
parser.add_argument('--n_epochs', required=True);
parser.add_argument('--batch_size', required=True);
parser.add_argument('--filename', required=True);

args = parser.parse_args()

def normalize_scores(dataset, num_classes):

  normalized = dict();
  classes = dict();
  for i in range(1, 8):
    max_score = -float('inf')
    min_score = float('inf');
    for j in range(len(dataset[str(i)])):
      sample_score = dataset[str(i)][j]['score'];
      if(sample_score > max_score):
        max_score = sample_score;
      if(sample_score < min_score):
        min_score = sample_score;
    datalist = []
    for k in range(len(dataset[str(i)])):
      if(num_classes is not None):
        new_score = int((dataset[str(i)][k]['score'] - min_score)/(max_score - min_score) * num_classes)
      else:
        new_score = dataset[str(i)][k]['score']
      datalist.append({'essay':dataset[str(i)][k]['essay'], 'score':new_score});
    normalized[str(i)] = datasets.Dataset.from_pandas(pd.DataFrame(data=datalist))
    classes[str(i)] = max_score if num_classes is None else num_classes
  return normalized, classes

dataset = load_dataset("Ericwang/ASAP")
num_classes = args.n_classes;
dataset, classes = normalize_scores(dataset, num_classes);
prompt = str(args.prompt);
sz = len(dataset[prompt])
train_sz, val_sz, test_sz = math.floor(0.5*sz), math.floor(0.25*sz), math.floor(0.25*sz)
num_classes = classes[prompt]
dataset = DatasetDict(
    train=dataset[prompt].shuffle(seed=1111).select(range(train_sz)),
    val=dataset[prompt].shuffle(seed=1111).select(range(val_sz)),
    test=dataset[prompt].shuffle(seed=1111).select(range(test_sz)),
)

batch_size = int(args.batch_size);
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
tokenized_dataset = dataset.map(
    lambda example: tokenizer(example['essay'], padding=True, truncation=True),
    batched=True,
    batch_size=batch_size
)
tokenized_dataset = tokenized_dataset.remove_columns(["essay"])
tokenized_dataset = tokenized_dataset.rename_column("score", "labels")
tokenized_dataset.set_format("torch")

train_dataloader = DataLoader(tokenized_dataset['train'], batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True);
eval_dataloader = DataLoader(tokenized_dataset['val'], batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True);
test_dataloader = DataLoader(tokenized_dataset['test'], batch_size=1)

class BaselineModel(torch.nn.Module):
  def __init__(self,num_labels=2):
    super(BaselineModel,self).__init__() 
    self.num_labels = num_labels 


    self.dropout = nn.Dropout(0.1) 

    # Configure DistilBERT's initialization
    config = DistilBertConfig(output_hidden_states=True)

    self.model = model = DistilBertModel.from_pretrained("distilbert-base-uncased", config=config)
    #for param in self.model.parameters():
    #  param.requires_grad = False

    self.classifier = nn.Linear(768, num_labels) # load and initialize weights

  def forward(self, input_ids=None, attention_mask=None, labels=None):
    #Extract outputs from the body
    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)

    #Add custom layers
    sequence_output = self.dropout(outputs[0]) #outputs[0]=last hidden state

    logits = self.classifier(sequence_output[:,0,:].view(-1,768)) # calculate losses
    
    loss = None
    if labels is not None:
      loss_fct = nn.CrossEntropyLoss()
      loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
    
    return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states,attentions=outputs.attentions)


num_epochs = int(args.n_epochs);
num_training_steps = num_epochs * len(train_dataloader)
model = BaselineModel(num_classes)
model = model.to(device);
optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)
lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)

best_val_loss = float("inf")

exp_name = args.filename
progress_bar = tqdm(range(num_training_steps))

train_loss = [];
val_loss = [];

for epoch in range(num_epochs):
  # training
  model.train()
  train_loss = 0;
  for batch_i, batch in enumerate(train_dataloader):
      batch = {k: v.to(device) for k, v in batch.items()}
      # batch = ([text1, text2], [0, 1])
      output = model(**batch)
      
      optimizer.zero_grad()
      train_loss += output.loss;
      output.loss.backward()
      optimizer.step()
      lr_scheduler.step()
      progress_bar.update(1)

  avg_train_loss = train_loss/len(train_dataloader);
  # validation
  model.eval()
  val_loss = 0;
  for batch_i, batch in enumerate(eval_dataloader):
      batch = {k: v.to(device) for k, v in batch.items()}
      with torch.no_grad():
          output = model(**batch)
      val_loss += output.loss
  
  avg_val_loss = val_loss / len(eval_dataloader)
  train_loss.append(avg_train_loss);
  val_loss.append(avg_val_loss);
  print(f"Epoch {epoch+1}" + " | " + f"Training loss: {avg_train_loss}" + " | " + f"Validation loss: {avg_val_loss}")
  if avg_val_loss < best_val_loss:
      print("Saving checkpoint!")
      best_val_loss = avg_val_loss
      torch.save({
          'epoch': epoch,
          'model_state_dict': model.state_dict(),
          'optimizer_state_dict': optimizer.state_dict(),
          'val_loss': best_val_loss,
          },
          f"/checkpoints/epoch_{epoch}.pt"
      )

model.eval()
predictions_test = [];
labels_test = [];
for batch_i, batch in enumerate(test_dataloader):
  batch = {k: v.to(device) for k, v in batch.items()}
  with torch.no_grad():
      output = model(**batch)
  loss = output.loss;
  pred = np.array(np.argmax((output.logits).cpu()))
  truth = np.array(batch['labels'].cpu())[0]
  predictions_test.append(pred);
  labels_test.append(truth);

exp_meta = dict();
exp_meta.results = {'testing': {'labels':labels_test, "predictions": predictions_test},
                  'training':train_loss, 'validation':val_loss}
exp_meta.data = {'prompt':prompt, 'training': train_sz, "validation":val_sz, "testing":test_sz}
exp_meta.training_params = {'num_epochs':num_epochs, 'batch_size':batch_size, 'num_classes':num_classes}


with open(exp_name + '.pickle', 'wb') as handle:
  pickle.dump(exp_meta, handle)


